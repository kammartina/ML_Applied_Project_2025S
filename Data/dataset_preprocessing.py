# -*- coding: utf-8 -*-
"""Dataset_Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13lTsqPOhHC7ZEeR0xdUmF_qeK8IJrzox

# Translation of TED Talk scripts

**Pre-trained model:** “google-t5/t5-base“ -  https://huggingface.co/google-t5/t5-base#uses
- T5 documentation: https://huggingface.co/docs/transformers/model_doc/t5

**Dataset:** "TED Talks Web-scraped dataset" - https://www.kaggle.com/datasets/jeniagerasimov/ted-talks-info-dataset

# Setup
"""

!pip install datasets

from pprint import pprint

# Mounting Google Drive w
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""# Dataset "IWSLT/ted_talks_iwslt"

- source: https://huggingface.co/datasets/IWSLT/ted_talks_iwslt
- en, de
<br><br>

**The full list of languages (109 lang.):** 'af', 'am', 'ar', 'arq', 'art-x-bork', 'as', 'ast', 'az', 'be', 'bg', 'bi', 'bn', 'bo', 'bs', 'ca', 'ceb', 'cnh', 'cs', 'da', 'de', 'el', 'en', 'eo', 'es', 'et', 'eu', 'fa', 'fi', 'fil', 'fr', 'fr-ca', 'ga', 'gl', 'gu', 'ha', 'he', 'hi', 'hr', 'ht', 'hu', 'hup', 'hy', 'id', 'ig', 'inh', 'is', 'it', 'ja', 'ka', 'kk', 'km', 'kn', 'ko', 'ku', 'ky', 'la', 'lb', 'lo', 'lt', 'ltg', 'lv', 'mg', 'mk', 'ml', 'mn', 'mr', 'ms', 'mt', 'my', 'nb', 'ne', 'nl', 'nn', 'oc', 'pa', 'pl', 'ps', 'pt', 'pt-br', 'ro', 'ru', 'rup', 'sh', 'si', 'sk', 'sl', 'so', 'sq', 'sr', 'srp', 'sv', 'sw', 'szl', 'ta', 'te', 'tg', 'th', 'tl', 'tlh', 'tr', 'tt', 'ug', 'uk', 'ur', 'uz', 'vi', 'zh', 'zh-cn', 'zh-tw'.
<br><br>

**Original data files:**
- original webpage for download: https://wit3.fbk.eu/home
- **For each language, a single XML file is generated which includes all talks subtitled in that language.**
- The original XML files are formatted like this example:


```
<file id="1">
  <head>
    <url>http://www.ted.com/talks/ryan_holladay_to_hear_this_music_you_have_to_be_there_literally.html</url>
    <pagesize>66634</pagesize>
    <dtime>Sun Jan 12 15:17:32 CET 2014</dtime>
    <content-type>text/html; charset=utf-8</content-type>
    <encoding>utf-8</encoding>
    <videourl>http://download.ted.com/talks/RyanHolladay_2013S.mp4</videourl>
    <videopath>talks/RyanHolladay_2013S.mp4</videopath>
    <transcription>
      <seekvideo id="2939">(Music)</seekvideo>
      <seekvideo id="7555">For any of you who have visited or lived in New York City,</seekvideo>
      <seekvideo id="11221">these shots might start to look familiar.</seekvideo>
      <seekvideo id="16116">This is Central Park,</seekvideo>
      .
      .
      .
      <seekvideo id="361992">for people to interact with</seekvideo>
      <seekvideo id="363709">and experience music.</seekvideo>
      <seekvideo id="365451">Thank you.</seekvideo>
      <seekvideo id="367495">(Applause)</seekvideo>
    </transcription>
    <talkid>1903</talkid>
    <title>Ryan Holladay: To hear this music you have to be there. Literally</title>
    <description>The music industry ......segments of sounds that only play when a listener is physically nearby. (Filmed at TED@BCG.)</description>
    <keywords>entertainment,music,technology</keywords>
    <image>http://images.ted.com/images/ted/d98c17773da6f84e9f915895c270c7ffd2de3778_389x292.jpg</image>
    <date>2014/01/12</date>
    <wordnum>885</wordnum>
    <charnum>5051</charnum>
  </head>
  <content>(Music) For any of you who have visited or lived in New York City, these shots might start to look familiar. This is Central Park, ............new ways for people to interact with and experience music. Thank you. (Applause)</content>
</file>
```
--> we need to extract:
- `<talkid>`
- `<content>`

## Load XLM files

Dataset: https://huggingface.co/datasets/IWSLT/ted_talks_iwslt

## XML to CSV - en
"""

import zipfile
import xml.etree.ElementTree as ET
import pandas as pd

file_path_xml_en = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_xml_en.zip'

# Step 1: Open the ZIP file
with zipfile.ZipFile(file_path_xml_en, 'r') as zip_ref:
    # List all files inside the ZIP to locate the XML filename
    xml_filenames = zip_ref.namelist()
    print("Files inside ZIP:", xml_filenames)

    # Assume the first XML file is the one we want (you can change this if needed)
    xml_filename = xml_filenames[0]

    # Step 2: Read XML file content from ZIP without extracting
    with zip_ref.open(xml_filename) as xml_file:
        tree = ET.parse(xml_file)
        root = tree.getroot()

        # Step 3: Extract talk_id and content from each <file> element
        data = []
        for file_elem in root.findall('file'):
            talk_id = file_elem.findtext('.//talkid')
            content = file_elem.findtext('.//content')
            if talk_id and content:
                content = content.strip().replace('\n', ' ')
                data.append([talk_id, content])

# Step 4: Create a DataFrame
df_en = pd.DataFrame(data, columns=['talk_id', 'content'])

# Step 5: Save the CSV to Google Drive
csv_output_path_en = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_csv_en.csv'
df_en.to_csv(csv_output_path_en, index=False)

print(f"CSV file saved to: {csv_output_path_en}")

print(df_en.head())

pprint(df_en['content'][0])

"""## XML to CSV - de"""

file_path_xml_de = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_xml_de.zip'

# Step 1: Open the ZIP file
with zipfile.ZipFile(file_path_xml_de, 'r') as zip_ref:
    # List all files inside the ZIP to locate the XML filename
    xml_filenames = zip_ref.namelist()
    print("Files inside ZIP:", xml_filenames)

    # Assume the first XML file is the one we want (you can change this if needed)
    xml_filename = xml_filenames[0]

    # Step 2: Read XML file content from ZIP without extracting
    with zip_ref.open(xml_filename) as xml_file:
        tree = ET.parse(xml_file)
        root = tree.getroot()

        # Step 3: Extract talk_id and content from each <file> element
        data = []
        for file_elem in root.findall('file'):
            talk_id = file_elem.findtext('.//talkid')
            content = file_elem.findtext('.//content')
            if talk_id and content:
                content = content.strip().replace('\n', ' ')
                data.append([talk_id, content])

# Step 4: Create a DataFrame
df_de = pd.DataFrame(data, columns=['talk_id', 'content'])

# Step 5: Save the CSV to Google Drive
csv_output_path_de = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_csv_de.csv'
df_de.to_csv(csv_output_path_de, index=False)

print(f"CSV file saved to: {csv_output_path_de}")

print(df_de.head())

pprint(df_de['content'][0])

"""## Intersection between EN and DE talks"""

# Step 1: Load English and German CSVs
english_csv_path = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_csv_en.csv'
german_csv_path = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_csv_de.csv'

df_en = pd.read_csv(english_csv_path)
df_de = pd.read_csv(german_csv_path)

# Step 2: Merge DataFrames on 'talk_id' (inner join keeps only matched pairs)
merged_df = pd.merge(df_en, df_de, on='talk_id', how='inner', suffixes=('_en', '_de'))

# Step 3: Drop rows with any missing values in any column
merged_df = merged_df.dropna()

# Step 4: Save the merged file
merged_output_path = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_en_de.csv'
merged_df.to_csv(merged_output_path, index=False)

print(f"Merged CSV file saved to: {merged_output_path}")

# Show number of matching samples
num_samples = len(merged_df)
print(f"Number of matched English-German TED talks: {num_samples}")

# Show the first sample (row) of the merged DataFrame
first_sample = merged_df.iloc[0]

print("First matched English-German TED Talk:")
print(f"TALK ID: {first_sample['talk_id']}\n")
print("ENGLISH CONTENT:\n", first_sample['content_en'], "\n")
print("GERMAN CONTENT:\n", first_sample['content_de'])

"""## XML to CSV - sk"""

file_path_xml_sk = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_xml_sk.zip'

# Step 1: Open the ZIP file
with zipfile.ZipFile(file_path_xml_sk, 'r') as zip_ref:
    # List all files inside the ZIP to locate the XML filename
    xml_filenames = zip_ref.namelist()
    print("Files inside ZIP:", xml_filenames)

    # Assume the first XML file is the one we want (you can change this if needed)
    xml_filename = xml_filenames[0]

    # Step 2: Read XML file content from ZIP without extracting
    with zip_ref.open(xml_filename) as xml_file:
        tree = ET.parse(xml_file)
        root = tree.getroot()

        # Step 3: Extract talk_id and content from each <file> element
        data = []
        for file_elem in root.findall('file'):
            talk_id = file_elem.findtext('.//talkid')
            content = file_elem.findtext('.//content')
            if talk_id and content:
                content = content.strip().replace('\n', ' ')
                data.append([talk_id, content])

# Step 4: Create a DataFrame
df_sk = pd.DataFrame(data, columns=['talk_id', 'content'])

# Step 5: Save the CSV to Google Drive
csv_output_path_sk = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_csv_sk.csv'
df_sk.to_csv(csv_output_path_sk, index=False)

print(f"CSV file saved to: {csv_output_path_sk}")

print(df_sk.head())

pprint(df_sk['content'][0])

"""## XML to CSV - zh-cn"""

file_path_xml_zh_cn = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_xml_zh-cn.zip'

# Step 1: Open the ZIP file
with zipfile.ZipFile(file_path_xml_zh_cn, 'r') as zip_ref:
    # List all files inside the ZIP to locate the XML filename
    xml_filenames = zip_ref.namelist()
    print("Files inside ZIP:", xml_filenames)

    # Assume the first XML file is the one we want (you can change this if needed)
    xml_filename = xml_filenames[0]

    # Step 2: Read XML file content from ZIP without extracting
    with zip_ref.open(xml_filename) as xml_file:
        tree = ET.parse(xml_file)
        root = tree.getroot()

        # Step 3: Extract talk_id and content from each <file> element
        data = []
        for file_elem in root.findall('file'):
            talk_id = file_elem.findtext('.//talkid')
            content = file_elem.findtext('.//content')
            if talk_id and content:
                content = content.strip().replace('\n', ' ')
                data.append([talk_id, content])

# Step 4: Create a DataFrame
df_zh_cn = pd.DataFrame(data, columns=['talk_id', 'content'])

# Step 5: Save the CSV to Google Drive
csv_output_path_zh_cn = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_csv_zh-cn.csv'
df_zh_cn.to_csv(csv_output_path_zh_cn, index=False)

print(f"CSV file saved to: {csv_output_path_zh_cn}")

print(df_zh_cn.head())

pprint(df_zh_cn['content'][0])

"""## XML to CSV - it"""

file_path_xml_it = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_xml_it.zip'

# Step 1: Open the ZIP file
with zipfile.ZipFile(file_path_xml_it, 'r') as zip_ref:
    # List all files inside the ZIP to locate the XML filename
    xml_filenames = zip_ref.namelist()
    print("Files inside ZIP:", xml_filenames)

    # Assume the first XML file is the one we want (you can change this if needed)
    xml_filename = xml_filenames[0]

    # Step 2: Read XML file content from ZIP without extracting
    with zip_ref.open(xml_filename) as xml_file:
        tree = ET.parse(xml_file)
        root = tree.getroot()

        # Step 3: Extract talk_id and content from each <file> element
        data = []
        for file_elem in root.findall('file'):
            talk_id = file_elem.findtext('.//talkid')
            content = file_elem.findtext('.//content')
            if talk_id and content:
                content = content.strip().replace('\n', ' ')
                data.append([talk_id, content])

# Step 4: Create a DataFrame
df_it = pd.DataFrame(data, columns=['talk_id', 'content'])

# Step 5: Save the CSV to Google Drive
csv_output_path_it = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_csv_it.csv'
df_it.to_csv(csv_output_path_it, index=False)

print(f"CSV file saved to: {csv_output_path_it}")

print(df_it.head())

pprint(df_it['content'][0])

"""## XML to CSV - fr"""

file_path_xml_fr = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_xml_fr.zip'

# Step 1: Open the ZIP file
with zipfile.ZipFile(file_path_xml_fr, 'r') as zip_ref:
    # List all files inside the ZIP to locate the XML filename
    xml_filenames = zip_ref.namelist()
    print("Files inside ZIP:", xml_filenames)

    # Assume the first XML file is the one we want (you can change this if needed)
    xml_filename = xml_filenames[0]

    # Step 2: Read XML file content from ZIP without extracting
    with zip_ref.open(xml_filename) as xml_file:
        tree = ET.parse(xml_file)
        root = tree.getroot()

        # Step 3: Extract talk_id and content from each <file> element
        data = []
        for file_elem in root.findall('file'):
            talk_id = file_elem.findtext('.//talkid')
            content = file_elem.findtext('.//content')
            if talk_id and content:
                content = content.strip().replace('\n', ' ')
                data.append([talk_id, content])

# Step 4: Create a DataFrame
df_fr = pd.DataFrame(data, columns=['talk_id', 'content'])

# Step 5: Save the CSV to Google Drive
csv_output_path_fr = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_csv_fr.csv'
df_fr.to_csv(csv_output_path_fr, index=False)

print(f"CSV file saved to: {csv_output_path_fr}")

print(df_fr.head())

pprint(df_fr['content'][0])

"""## Intersection en, de, sk, zh-cn, it, fr"""

# Step 1: Load English and German CSVs
en_csv_path = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_csv_en.csv'
de_csv_path = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_csv_de.csv'
sk_csv_path = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_csv_sk.csv'
zh_cn_csv_path = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_csv_zh-cn.csv'
it_csv_path = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_csv_it.csv'
fr_csv_path = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_csv_fr.csv'

df_en = pd.read_csv(english_csv_path)
df_de = pd.read_csv(german_csv_path)
df_sk = pd.read_csv(sk_csv_path)
df_zh_cn = pd.read_csv(zh_cn_csv_path)
df_it = pd.read_csv(it_csv_path)
df_fr = pd.read_csv(fr_csv_path)

# OPTIONAL: Rename content columns before merging to avoid confusion
df_en = df_en.rename(columns={'content': 'content_en'})
df_de = df_de.rename(columns={'content': 'content_de'})
df_sk = df_sk.rename(columns={'content': 'content_sk'})
df_zh_cn = df_zh_cn.rename(columns={'content': 'content_zh_cn'})
df_it = df_it.rename(columns={'content': 'content_it'})
df_fr = df_fr.rename(columns={'content': 'content_fr'})

# Step 2: Merge DataFrames on 'talk_id' (inner join keeps only matched pairs)
# Merge DataFrames one at a time on 'talk_id'
merged_df_big = df_en
merged_df_big = pd.merge(merged_df_big, df_de, on='talk_id', how='inner')
merged_df_big = pd.merge(merged_df_big, df_sk, on='talk_id', how='inner')
merged_df_big = pd.merge(merged_df_big, df_zh_cn, on='talk_id', how='inner')
merged_df_big = pd.merge(merged_df_big, df_it, on='talk_id', how='inner')
merged_df_big = pd.merge(merged_df_big, df_fr, on='talk_id', how='inner')

# Step 3: Drop rows with any missing values in any column
merged_df_big = merged_df_big.dropna()

# Step 4: Save the merged file
merged_big_output_path = '/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_en_de_sk_zh-cn_it_fr.csv'
merged_df_big.to_csv(merged_big_output_path, index=False)

print(f"Merged CSV file saved to: {merged_big_output_path}")

# Show number of matching samples
num_samples_big = len(merged_df_big)
print(f"Number of matched TED talks: {num_samples_big}")

# Show the first sample (row) of the merged DataFrame
first_sample_big = merged_df_big.iloc[0]

print("First matched multilingual TED Talk sample:")
print(f"TALK ID: {first_sample_big['talk_id']}\n")

print("ENGLISH CONTENT:\n", first_sample_big['content_en'], "\n")
print("GERMAN CONTENT:\n", first_sample_big['content_de'], "\n")
print("SLOVAK CONTENT:\n", first_sample_big['content_sk'], "\n")
print("CHINESE (Simplified) CONTENT:\n", first_sample_big['content_zh_cn'], "\n")
print("ITALIAN CONTENT:\n", first_sample_big['content_it'], "\n")
print("FRENCH CONTENT:\n", first_sample_big['content_fr'], "\n")

"""---

## Extracting Sentence Pairs ("transcription")

We have TED Talk transcripts in multiple languages. **Each language** is stored in its **own ZIP file**, and **each ZIP contains one XML file per talk**.

Inside each XML file, the `<transcription>` section has **sentence-level or phrase-level segments**:

```
<seekvideo id="760">When I was first learning to meditate,</seekvideo>
```
The same `<seekvideo id>` **appears in the XML for the same TED talk in other languages**, so these are **aligned sentence pairs** (great for training a translation model).
<br><br>

GOAL:
Extract **sentence-level aligned pairs** like this:

```
{
  "2420": [
    {
      "en": "When I was first learning to meditate,",
      "de": "Als ich mit dem Meditieren begann,"
    },
    {
      "en": "the instruction was to simply pay attention to my breath,",
      "de": "war die Anweisung, einfach auf meinen Atem zu achten,"
    },
    ...
  ]
}
```

## English zip XML -> JSON
"""

import xml.etree.ElementTree as ET
import json
import zipfile

def extract_ted_transcriptions_from_zip(zip_path, output_path):
    all_talks = {}

    # Open the zip file
    with zipfile.ZipFile(zip_path, 'r') as zipf:
        # List XML files inside
        xml_files = [f for f in zipf.namelist() if f.endswith('.xml')]

        for xml_file in xml_files:
            with zipf.open(xml_file) as f:
                tree = ET.parse(f)
                root = tree.getroot()

                for file_elem in root.findall("file"):
                    talk_id_elem = file_elem.find(".//talkid")
                    transcription_elem = file_elem.find(".//transcription")

                    if talk_id_elem is not None and transcription_elem is not None:
                        talk_id = talk_id_elem.text.strip()
                        segments = [
                            elem.text.strip()
                            for elem in transcription_elem.findall("seekvideo")
                            if elem.text and elem.text.strip()
                        ]
                        if segments:
                            all_talks[talk_id] = segments

    # Save to JSON
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_talks, f, ensure_ascii=False, indent=2)

    print(f"Extracted {len(all_talks)} talks and saved to {output_path}")

input_path_en = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_xml_en.zip"
output_path_en = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_segments_en.json"

# Example usage
extract_ted_transcriptions_from_zip(input_path_en, output_path_en)

"""## German zip XML -> JSON"""

input_path_de = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_xml_de.zip"
output_path_de = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_segments_de.json"

# Example usage
extract_ted_transcriptions_from_zip(input_path_de, output_path_de)

"""## Align sentences (by index)"""

import json

def align_ted_translations(en_json_path, de_json_path, output_path):
    # Load English and German TED transcriptions
    with open(en_json_path, "r", encoding="utf-8") as f:
        en_data = json.load(f)

    with open(de_json_path, "r", encoding="utf-8") as f:
        de_data = json.load(f)

    aligned_data = {}

    # Find common talk IDs
    shared_ids = set(en_data.keys()) & set(de_data.keys())

    for talk_id in shared_ids:
        en_sentences = en_data[talk_id]
        de_sentences = de_data[talk_id]

        # Align sentence-by-sentence using zip (stops at shorter length)
        aligned_segments = [
            {"en": en, "de": de}
            for en, de in zip(en_sentences, de_sentences)
        ]

        aligned_data[talk_id] = aligned_segments

    # Save aligned data to output JSON
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(aligned_data, f, ensure_ascii=False, indent=2)

    print(f"Aligned {len(aligned_data)} talks and saved to {output_path}")

en_json = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_segments_en.json"
de_json = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_segments_de.json"
output_json = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_aligned_en_de.json"

align_ted_translations(en_json, de_json, output_json)

"""## Aligned en and de with key "translation"

In order to work with the prefix "translate from {source} to {target}: " which is essential for T5 model
"""

import json

# 1. load your existing file
input_path = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_aligned_en_de.json"
with open(input_path, "r", encoding="utf-8") as f:
    data = json.load(f)

# 2. open output JSONL and write each example
output_path = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_en_de_translation.json"
count = 0
with open(output_path, "w", encoding="utf-8") as out:
    for sentences in data.values():
        for pair in sentences:
            example = {
                "translation": {
                    "en": pair["en"],
                    "de": pair["de"]
                }
            }
            json.dump(example, out, ensure_ascii=False)
            out.write("\n")
            count += 1

print(f"Wrote {count} translation examples to {output_path}")

import json

# 1) paths
input_path  = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_aligned_en_de.json"
output_path = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_en_de_translation2.json"

# 2) load your per‑talk file
with open(input_path, "r", encoding="utf-8") as f:
    data = json.load(f)

# 3) flatten into a list of {"translation": {"en":…, "de":…}}
flat = [
    {"translation": {"en": pair["en"], "de": pair["de"]}}
    for sentences in data.values()
    for pair in sentences
]

# 4) write out as one big JSON array
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(flat, f, ensure_ascii=False, indent=2)

print(f"Saved {len(flat)} translation examples to {output_path}")

"""## Direction token

Adding a direction token like `"to_de"` or `"to_en"` is a great idea, especially if you're **building a multilingual translation model** that needs **to handle multiple directions with a shared architecture** (such as in a many-to-many or many-to-one setup). This token helps the model understand which language it's supposed to generate.

**Why Add a Direction Token?**

- Disambiguation: The model learns from both directions. Without a direction token, it won't know which language to translate into.
- Scalability: When you add more languages later (e.g., French, Spanish), the same mechanism works. Just add `"to_fr"`, `"to_es"`, etc.
- Consistency: It aligns with how popular multilingual models like mBART, mT5, or fairseq models handle multiple language pairs.

We have now this:


```
{
  "2420": [
    {
      "en": "When I was first learning to meditate,",
      "de": "Als ich mit dem Meditieren begann,"
    },
    ...
  ]
}
```

Now you want to turn that into training-ready parallel sentence pairs like:

- `"to_de <English sentence>" → <German sentence>`
- `"to_en <German sentence>" → <English sentence>`

**When Should You Use This?**

-->Right before training, when you need to create your final dataset for input/output.

Works well for training with tools like:
- Hugging Face Transformers (via datasets)
- Fairseq
- MarianMT
- OpenNMT
- Custom PyTorch/TF models
"""

# Step 1: Load your aligned JSON file

with open(output_json, "r", encoding="utf-8") as f:
    aligned_data = json.load(f)

# Step 2: Create parallel sentence pairs with to_xx tokens

def prepare_parallel_data(aligned_data, direction="en_to_de"):
    pairs = []

    for talk_id, segments in aligned_data.items():
        for pair in segments:
            if direction == "en_to_de":
                src = f"to_de {pair['en']}"
                tgt = pair['de']
            elif direction == "de_to_en":
                src = f"to_en {pair['de']}"
                tgt = pair['en']
            else:
                continue  # Ignore unsupported directions
            pairs.append((src, tgt))

    return pairs

# Save the result for training
# You can save it as a simple tab-separated .tsv or .txt file for use in most translation model pipelines.

# Choose direction
direction = "en_to_de"  # or "de_to_en"

# Prepare pairs
pairs = prepare_parallel_data(aligned_data, direction=direction)

# Save to file
output_file = f"parallel_{direction}.tsv"
with open(output_file, "w", encoding="utf-8") as f:
    for src, tgt in pairs:
        f.write(f"{src}\t{tgt}\n")

print(f"Saved {len(pairs)} sentence pairs to {output_file}")

"""## French zip XML -> JSON"""

input_path_fr = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_xml_fr.zip"
output_path_fr = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_segments_fr.json"

# Example usage
extract_ted_transcriptions_from_zip(input_path_fr, output_path_fr)

"""## Alinged EN and FR"""

import json

def align_en_fr_ted_translations(en_json_path, fr_json_path, output_path):
    # Load English and German TED transcriptions
    with open(en_json_path, "r", encoding="utf-8") as f:
        en_data = json.load(f)

    with open(fr_json_path, "r", encoding="utf-8") as f:
        fr_data = json.load(f)

    aligned_data = {}

    # Find common talk IDs
    shared_ids = set(en_data.keys()) & set(fr_data.keys())

    for talk_id in shared_ids:
        en_sentences = en_data[talk_id]
        fr_sentences = fr_data[talk_id]

        # Align sentence-by-sentence using zip (stops at shorter length)
        aligned_segments = [
            {"en": en, "fr": fr}
            for en, fr in zip(en_sentences, fr_sentences)
        ]

        aligned_data[talk_id] = aligned_segments

    # Save aligned data to output JSON
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(aligned_data, f, ensure_ascii=False, indent=2)

    print(f"Aligned {len(aligned_data)} talks and saved to {output_path}")

en_json = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_segments_en.json"
fr_json = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_segments_fr.json"
output_json_en_fr = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_aligned_en_fr.json"

align_en_fr_ted_translations(en_json, fr_json, output_json_en_fr)

import json

# 1. load your existing file
input_path = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_aligned_en_fr.json"
with open(input_path, "r", encoding="utf-8") as f:
    data = json.load(f)

# 2. open output JSONL and write each example
output_path = "/content/drive/MyDrive/ML2_Bublin_Project/Multilingual_IWSLT/ted_en_fr_translation.json"
count = 0
with open(output_path, "w", encoding="utf-8") as out:
    for sentences in data.values():
        for pair in sentences:
            example = {
                "translation": {
                    "en": pair["en"],
                    "fr": pair["fr"]
                }
            }
            json.dump(example, out, ensure_ascii=False)
            out.write("\n")
            count += 1

print(f"Wrote {count} translation examples to {output_path}")

"""---

# Exploring other multilingual datasets

## Dataset "neulab/ted_multi"

Source: https://huggingface.co/datasets/neulab/ted_multi
"""

new_dataset_2 = load_dataset('neulab/ted_multi', name="plain_text")

print(new_dataset_2)

pprint(new_dataset_2['train'][0])

"""---

## Dataset "davidstap/ted_talks"

Source: https://huggingface.co/datasets/davidstap/ted_talks
"""

from datasets import load_dataset

new_dataset = load_dataset('davidstap/ted_talks', "en_de", trust_remote_code=True)

print(new_dataset)

pprint(new_dataset['train'][:10])