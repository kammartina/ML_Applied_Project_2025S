# -*- coding: utf-8 -*-
"""cleaning_splits_with_bicleaner_Jessica.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13LISdB8JdqGl1Mo-aSaaGpKrHViHTgtb

# Setup
"""

!apt install libhunspell-dev
!apt-get install hunspell-en-us
!apt-get install hunspell-de-de

!pip install cyhunspell

!pip install bicleaner-hardrules
!pip install --config-settings="--build-option=--max_order=7" https://github.com/kpu/kenlm/archive/master.zip
!pip install numpy==1.24

import os
import json

"""# Define needed functions"""

# Define preprocessing and tracking
def escape_special_chars(text):
    """Escapes special characters that might cause problems for bicleaner."""
    edit_counts = {
        "tabs_replaced": 0,
        "backslashes_escaped": 0,
        "quotes_escaped": 0,
        "newlines_escaped": 0,
        "carriage_returns_removed": 0,
    }

    # Replace tabs with spaces
    edit_counts["tabs_replaced"] = text.count('\t')
    text = text.replace("\t", " ")

    # Escape backslashes
    edit_counts["backslashes_escaped"] = text.count('\\')
    text = text.replace("\\", "\\\\")

    # Escape quotes
    edit_counts["quotes_escaped"] = text.count('"')
    text = text.replace('"', '\\"')

    # Escape newlines
    edit_counts["newlines_escaped"] = text.count('\n')
    text = text.replace('\n', '\\n')

    # Remove carriage returns
    edit_counts["carriage_returns_removed"] = text.count('\r')
    text = text.replace('\r', '')

    return text, edit_counts  # Return both the modified text and the edit counts dictionary

def jsonl2tsv(input_jsonl, tsv_path):
  with open(input_jsonl, "r", encoding="utf-8") as infile, open(tsv_path, "w", encoding="utf-8") as outfile:
      total_edit_counts = {
          "tabs_replaced": 0,
          "backslashes_escaped": 0,
          "quotes_escaped": 0,
          "newlines_escaped": 0,
          "carriage_returns_removed": 0,
      }  # Initialize total edit counts dictionary
      for i, line in enumerate(infile):
          # Attempt to load the JSON, handling errors
          try:
              obj = json.loads(line)
          except json.JSONDecodeError as e:
              # Print the problematic line and the error for debugging
              print(f"Error decoding JSON: {e}")
              print(f"Problematic line: {line}")
              # Skip this line and continue processing the next line
              continue

          en, en_edit_counts = escape_special_chars(obj["translation"].get("en", "").strip())
          de, de_edit_counts = escape_special_chars(obj["translation"].get("de", "").strip())


          # Accumulate edits per rule
          for rule in total_edit_counts:
              total_edit_counts[rule] += en_edit_counts[rule] + de_edit_counts[rule]

            # Debugging: Print the first 10 lines to inspect the data
          if i < 10:
              print(f"Line {i}: en='{en}', de='{de}'")

          if en and de:
              outfile.write(f"{en}\t{de}\n")
          else:
              print(f"Skipping line {i} due to missing en or de translation.")


      # Print total edit counts per rule
      for rule, count in total_edit_counts.items():
          print(f"Total edits for {rule}: {count}")

def tsv2jsonl(clean_tsv, output_jsonl):
  with open(clean_tsv, "r", encoding="utf-8") as infile, open(output_jsonl, "w", encoding="utf-8") as outfile:
      for line in infile:
          parts = line.strip().split("\t")
          if len(parts) >= 2:
              en, de = parts[0], parts[1]
              json_obj = {"translation": {"en": en, "de": de}}
              outfile.write(json.dumps(json_obj, ensure_ascii=False) + "\n")

  with open(output_jsonl, "r", encoding="utf-8") as file:
      for line in file.readlines()[:10]:
        print(line)

"""#Bicleaning

## Training Set
"""

train_input_jsonl = "/content/train.jsonl"
train_tsv_path = "/content/train.en-de"

jsonl2tsv(train_input_jsonl, train_tsv_path)

# Run Bicleaner
!bicleaner-hardrules -s en -t de /content/train.en-de /content/train.classified

# Filter Good Pairs
!grep '1$' /content/train.classified > /content/train.clean

# Summary
!wc -l /content/train.en-de
!wc -l /content/train.clean

# Convert Clean Output Back to JSONL
train_clean_jsonl = "/content/train.clean.jsonl"
train_clean_tsv = "/content/train.clean"

tsv2jsonl(train_clean_tsv, train_clean_jsonl)

"""## Validation Set"""

val_input_jsonl = "/content/val.jsonl"
val_tsv_path = "/content/val.en-de"

jsonl2tsv(val_input_jsonl, val_tsv_path)

# Run Bicleaner
!bicleaner-hardrules -s en -t de /content/val.en-de /content/val.classified

# Filter Good Pairs
!grep '1$' /content/val.classified > /content/val.clean

# Summary
!wc -l /content/val.en-de
!wc -l /content/val.clean

# Convert Clean Output Back to JSONL
val_clean_jsonl = "/content/val.clean.jsonl"
val_clean_tsv = "/content/val.clean"

tsv2jsonl(val_clean_tsv, val_clean_jsonl)

"""## Test Set"""

test_input_jsonl = "/content/test.jsonl"
test_tsv_path = "/content/test.en-de"

jsonl2tsv(test_input_jsonl, test_tsv_path)

# Run Bicleaner
!bicleaner-hardrules -s en -t de /content/test.en-de /content/test.classified

# Filter Good Pairs
!grep '1$' /content/test.classified > /content/test.clean

# Summary
!wc -l /content/test.en-de
!wc -l /content/test.clean

# Convert Clean Output Back to JSONL
test_clean_jsonl = "/content/test.clean.jsonl"
test_clean_tsv = "/content/test.clean"

tsv2jsonl(test_clean_tsv, test_clean_jsonl)