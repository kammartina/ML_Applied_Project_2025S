# -*- coding: utf-8 -*-
"""nllb_en-fr.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kbB_i9U8LzUldeu7qzMRn5TkUWaN-GwZ
"""

!pip install datasets transformers evaluate sacrebleu chrf

!pip install sacrebleu

!pip install --upgrade evaluate

import evaluate

from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer
)

#from nllb_data_loader import load_tokenized_dataset

from datasets import Dataset, DatasetDict, load_dataset

from tqdm.auto import tqdm # Import tqdm
import json

# Mounting Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define paths for each split
# the splits were uploaded into the Colab
train_path = "/content/en-fr_train.clean.jsonl"
val_path = "/content/en-fr_val.clean.jsonl"
test_path = "/content/en-fr_test.clean.jsonl"

# !pip install --upgrade datasets

# Model and tokenizer
checkpoint_path = "/content/drive/MyDrive/ML2_Bublin_Project/Sandra_NLLB/nllb_en-de_finetuned(32K)_cleaned_dataset/checkpoint-10000"

model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)
tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)

def load_tokenized_dataset(model_name="facebook/nllb-200-distilled-600M"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.src_lang = "eng_Latn"
    tokenizer.tgt_lang = "fra_Latn"

    def load_jsonl(path, size=None):
        with open(path, 'r', encoding='utf-8') as f:
            lines = [json.loads(line) for line in tqdm(f, desc=f"Loading {path}")]
        ###
        if size is not None and size > 0:
            return lines[:size]
        ###
        return lines

    def tokenize_pairs(pairs):
        src = [pair['translation']['en'] for pair in pairs]
        tgt = [pair['translation']['fr'] for pair in pairs]
        tokenized = tokenizer(src, text_target=tgt, padding=True, truncation=True)
        return Dataset.from_dict(tokenized)

    train_pairs = load_jsonl("en-fr_train.clean.jsonl", size=32000)
    val_pairs = load_jsonl("en-fr_val.clean.jsonl", size=512)
    test_pairs = load_jsonl("en-fr_test.clean.jsonl", size=512)

    train_dataset = tokenize_pairs(train_pairs)
    val_dataset = tokenize_pairs(val_pairs)
    test_dataset = tokenize_pairs(test_pairs)

    return train_dataset, val_dataset, test_dataset

print("Loading dataset...")
# Load tokenized + split dataset
train_dataset, val_dataset, test_dataset = load_tokenized_dataset()

print(train_dataset)
print(val_dataset)
print(test_dataset)

# NLLB-specific language codes
tokenizer.src_lang = "eng_Latn"
tokenizer.tgt_lang = "fra_Latn"

# Evaluation metrics
bleu = evaluate.load("sacrebleu")
chrf = evaluate.load("chrf") # Character n-gram F-score

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    return {
        "sacrebleu": bleu.compute(predictions=decoded_preds, references=decoded_labels)["score"],
        "chrf": chrf.compute(predictions=decoded_preds, references=decoded_labels)["score"]
    }

# Data collator
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="/content/drive/MyDrive/ML2_Bublin_Project/Sandra_NLLB/nllb_en-fr_finetuned(32K)_cleaned_dataset",
    eval_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=1e-5, #smaller to aboid overfitting
    num_train_epochs=5,
    predict_with_generate=True,
    fp16=True,  # if on supported GPU
    logging_dir="/content/drive/MyDrive/ML2_Bublin_Project/Sandra_NLLB/nllb_en-fr_finetuned(32K)_cleaned_dataset/logs",
    logging_steps=100,
    save_total_limit=5,
    remove_unused_columns=False, # Keep "en" and "de" columns
    disable_tqdm=False,
    report_to="none"
)

# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

print("Starting training...")
trainer.train()

# Evaluate on the validation set
print("Evaluating on validation set...")
val_results = trainer.predict(test_dataset=val_dataset)
print(val_results)

# Evaluate on the test set
print("Evaluating on test set...")

model_path_test = "/content/drive/MyDrive/ML2_Bublin_Project/Sandra_NLLB/nllb_en-fr_finetuned(32K)_cleaned_dataset/checkpoint-10000"

try:
  model_finetuned = AutoModelForSeq2SeqLM.from_pretrained(model_path_test)
  print("Model loaded successfully.")
  # tokenizer used the same, loaded above

  # Initialize the data collator with the correct model
  data_collator_test = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_finetuned)

  # Initialize the Trainer for test evaluation
  trainer_test = Seq2SeqTrainer(
    model=model_finetuned,
    args=training_args,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator_test,
    compute_metrics=compute_metrics
  )

  # Evaluate the model on the test dataset
  print("\nStarting evaluation on test set...")
  test_results = trainer_test.evaluate(eval_dataset=test_dataset)
  print("Test results:\n", test_results)

except OSError as e:
    print(f"Error loading model: {e}")
    print("Please ensure 'config.json', 'model.safetensors' (or 'pytorch_model.bin'), and 'generation_config.json' are directly in the specified local path.")

# I did not use this
"""
if __name__ == "__main__":
    print("Starting training...")
    trainer.train()
    print("Training finished.")
    print("Saving model and tokenizer...")
    save_dir = "/content/drive/MyDrive/ML2_Bublin_Project/Sandra_NLLB/nllb_en-de_finetuned(32K)_cleaned_dataset"
    trainer.save_model(save_dir)
    trainer.model.save_pretrained(save_dir)
    tokenizer.save_pretrained(save_dir)
    print("Training complete.")
    print("Model and tokenizer saved.")
"""